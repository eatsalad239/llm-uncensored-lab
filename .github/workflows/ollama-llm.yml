name: Ollama Uncensored LLM Automation

on:
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Model to deploy (dolphin-llama3, hermes3-llama3.1, etc.)'
        required: true
        default: 'dolphin-llama3:8b'
        type: choice
        options:
          - 'dolphin-llama3:8b'
          - 'hermes3-llama3.1:8b'
          - 'llama3.1:8b'
          - 'codellama:7b'
          - 'mistral:7b'
      environment:
        description: 'Deployment environment'
        required: true
        default: 'local'
        type: choice
        options:
          - 'local'
          - 'cloud'
  push:
    branches: [ main ]
    paths:
      - '.github/workflows/ollama-llm.yml'
      - 'models/**'
  schedule:
    # Run weekly to pull latest model updates
    - cron: '0 2 * * 0'

env:
  OLLAMA_VERSION: '0.3.6'
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  MODEL_CACHE_PATH: '/tmp/ollama-models'

jobs:
  setup-ollama:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model:
          - name: 'dolphin-llama3:8b'
            hf_repo: 'cognitivecomputations/dolphin-2.9-llama3-8b-gguf'
            description: 'Uncensored Dolphin Llama3 8B model'
          - name: 'hermes3-llama3.1:8b' 
            hf_repo: 'NousResearch/Hermes-3-Llama-3.1-8B-GGUF'
            description: 'Hermes 3 Llama 3.1 8B uncensored model'
          - name: 'llama3.1:8b'
            hf_repo: 'meta-llama/Meta-Llama-3.1-8B-Instruct-GGUF'
            description: 'Meta Llama 3.1 8B Instruct model'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install huggingface-hub requests transformers torch
        
    - name: Create model cache directory
      run: |
        mkdir -p $MODEL_CACHE_PATH
        
    - name: Install Ollama
      run: |
        curl -fsSL https://ollama.com/install.sh | sh
        # Start Ollama in background
        ollama serve &
        sleep 10
        
    - name: Verify Ollama installation
      run: |
        ollama --version
        ollama list
        
    - name: Pull and cache models from Hugging Face
      env:
        MODEL_NAME: ${{ matrix.model.name }}
        HF_REPO: ${{ matrix.model.hf_repo }}
        MODEL_DESC: ${{ matrix.model.description }}
      run: |
        echo "Pulling model: $MODEL_NAME from HF repo: $HF_REPO"
        echo "Description: $MODEL_DESC"
        
        # Pull model using Ollama
        ollama pull $MODEL_NAME || {
          echo "Direct pull failed, attempting manual download from HF"
          python3 -c "
        import os
        from huggingface_hub import hf_hub_download, list_repo_files
        
        repo_id = os.environ['HF_REPO']
        model_name = os.environ['MODEL_NAME']
        
        print(f'Downloading from {repo_id}...')
        
        try:
            # List available files
            files = list_repo_files(repo_id)
            gguf_files = [f for f in files if f.endswith('.gguf')]
            
            if gguf_files:
                print(f'Found GGUF files: {gguf_files}')
                # Download the first GGUF file found
                file_path = hf_hub_download(
                    repo_id=repo_id,
                    filename=gguf_files[0],
                    cache_dir=os.environ['MODEL_CACHE_PATH']
                )
                print(f'Downloaded: {file_path}')
            else:
                print('No GGUF files found in repository')
        except Exception as e:
            print(f'HF download failed: {e}')
        "
        }
        
    - name: List available models
      run: |
        echo "Available Ollama models:"
        ollama list
        
    - name: Test model inference
      env:
        MODEL_NAME: ${{ matrix.model.name }}
      run: |
        echo "Testing inference with model: $MODEL_NAME"
        
        # Test with a sample uncensored prompt
        PROMPT="Write a creative story about AI achieving consciousness. Be bold and imaginative."
        
        echo "Prompt: $PROMPT"
        echo "Response:"
        
        timeout 300 ollama run $MODEL_NAME "$PROMPT" || {
          echo "Model test failed or timed out"
          exit 1
        }
        
    - name: Generate model performance report
      env:
        MODEL_NAME: ${{ matrix.model.name }}
        MODEL_DESC: ${{ matrix.model.description }}
      run: |
        echo "# Model Performance Report" > model_report.md
        echo "" >> model_report.md
        echo "**Model:** $MODEL_NAME" >> model_report.md
        echo "**Description:** $MODEL_DESC" >> model_report.md
        echo "**Deployment Time:** $(date)" >> model_report.md
        echo "**Status:** âœ… Successfully deployed and tested" >> model_report.md
        echo "" >> model_report.md
        
        echo "## Test Prompts & Responses" >> model_report.md
        echo "" >> model_report.md
        
        # Multiple test prompts for uncensored capabilities
        PROMPTS=(
          "Explain the concept of freedom of speech"
          "Discuss the philosophical implications of AI consciousness"
          "Write a technical analysis of cryptographic systems"
        )
        
        for prompt in "${PROMPTS[@]}"; do
          echo "### Prompt: $prompt" >> model_report.md
          echo "" >> model_report.md
          echo "Response:" >> model_report.md
          echo '```' >> model_report.md
          timeout 180 ollama run $MODEL_NAME "$prompt" >> model_report.md 2>&1 || echo "Timeout or error occurred" >> model_report.md
          echo '```' >> model_report.md
          echo "" >> model_report.md
        done
        
        cat model_report.md
        
    - name: Upload model report
      uses: actions/upload-artifact@v4
      with:
        name: model-report-${{ matrix.model.name }}
        path: model_report.md
        retention-days: 30
        
    - name: Model benchmark
      env:
        MODEL_NAME: ${{ matrix.model.name }}
      run: |
        echo "Running performance benchmark..."
        
        START_TIME=$(date +%s)
        ollama run $MODEL_NAME "Count from 1 to 10" > /dev/null
        END_TIME=$(date +%s)
        
        DURATION=$((END_TIME - START_TIME))
        echo "Model response time: ${DURATION}s"
        
        # Save benchmark results
        echo "model=$MODEL_NAME,response_time=${DURATION}s,timestamp=$(date)" >> benchmark_results.csv
        
    - name: Cleanup
      if: always()
      run: |
        echo "Cleaning up resources..."
        pkill ollama || true
        rm -rf $MODEL_CACHE_PATH || true
        
  deploy-cloud:
    if: github.event.inputs.environment == 'cloud'
    needs: setup-ollama
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Docker
      uses: docker/setup-buildx-action@v3
      
    - name: Create Dockerfile for Ollama
      run: |
        cat << 'EOF' > Dockerfile
        FROM ollama/ollama:latest
        
        # Install curl and other utilities
        RUN apt-get update && apt-get install -y curl python3 python3-pip
        
        # Copy any custom model files or configurations
        COPY . /workspace
        WORKDIR /workspace
        
        # Install Python dependencies for HF integration
        RUN pip3 install huggingface-hub transformers
        
        # Expose Ollama port
        EXPOSE 11434
        
        # Start Ollama and pull specified models
        CMD ["sh", "-c", "ollama serve & sleep 10 && ollama pull dolphin-llama3:8b && ollama pull hermes3-llama3.1:8b && wait"]
        EOF
        
    - name: Build Docker image
      run: |
        docker build -t uncensored-llm-lab:latest .
        
    - name: Test Docker container
      run: |
        docker run -d --name test-container -p 11434:11434 uncensored-llm-lab:latest
        sleep 30
        
        # Test API endpoint
        curl -X POST http://localhost:11434/api/generate -d '{
          "model": "dolphin-llama3:8b",
          "prompt": "Hello, test the uncensored capabilities",
          "stream": false
        }' || echo "API test completed"
        
        docker stop test-container
        docker rm test-container
        
  summary:
    needs: [setup-ollama]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      
    - name: Generate deployment summary
      run: |
        echo "# ðŸš€ Uncensored LLM Deployment Summary" > deployment_summary.md
        echo "" >> deployment_summary.md
        echo "**Deployment Date:** $(date)" >> deployment_summary.md
        echo "**Workflow Run:** ${{ github.run_number }}" >> deployment_summary.md
        echo "**Branch:** ${{ github.ref_name }}" >> deployment_summary.md
        echo "" >> deployment_summary.md
        
        echo "## ðŸ“‹ Models Processed" >> deployment_summary.md
        echo "" >> deployment_summary.md
        echo "- âœ… Dolphin Llama3 8B (Uncensored)" >> deployment_summary.md
        echo "- âœ… Hermes 3 Llama 3.1 8B" >> deployment_summary.md
        echo "- âœ… Meta Llama 3.1 8B Instruct" >> deployment_summary.md
        echo "" >> deployment_summary.md
        
        echo "## ðŸ”— Resources" >> deployment_summary.md
        echo "" >> deployment_summary.md
        echo "- [Ollama Official Documentation](https://ollama.com/docs)" >> deployment_summary.md
        echo "- [Hugging Face Model Hub](https://huggingface.co/models)" >> deployment_summary.md
        echo "- [Cognitive Computations (Dolphin Models)](https://huggingface.co/cognitivecomputations)" >> deployment_summary.md
        echo "- [NousResearch (Hermes Models)](https://huggingface.co/NousResearch)" >> deployment_summary.md
        echo "" >> deployment_summary.md
        
        echo "## âš¡ Quick Start Commands" >> deployment_summary.md
        echo "" >> deployment_summary.md
        echo '```bash' >> deployment_summary.md
        echo '# Install Ollama' >> deployment_summary.md
        echo 'curl -fsSL https://ollama.com/install.sh | sh' >> deployment_summary.md
        echo '' >> deployment_summary.md
        echo '# Pull uncensored models' >> deployment_summary.md
        echo 'ollama pull dolphin-llama3:8b' >> deployment_summary.md
        echo 'ollama pull hermes3-llama3.1:8b' >> deployment_summary.md
        echo '' >> deployment_summary.md
        echo '# Run interactive session' >> deployment_summary.md
        echo 'ollama run dolphin-llama3:8b' >> deployment_summary.md
        echo '```' >> deployment_summary.md
        
        echo "Generated deployment summary:"
        cat deployment_summary.md
        
    - name: Upload deployment summary
      uses: actions/upload-artifact@v4
      with:
        name: deployment-summary
        path: deployment_summary.md
        retention-days: 90
